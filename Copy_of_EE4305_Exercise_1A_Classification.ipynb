{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of EE4305: Exercise 1A - Classification",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhaozhijie1997/web/blob/master/Copy_of_EE4305_Exercise_1A_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtkP0kNnV-A1",
        "colab_type": "text"
      },
      "source": [
        "### Project Allocation\n",
        "\n",
        "Enter your matric number and run the following cell (shift + enter) to check project allocated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga9NIEqWQE6F",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "def95d3e-64dc-4572-b2aa-2d72702ddd0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "matric_number = \"A0150102H\" #@param {type:\"string\"}\n",
        "\n",
        "try:\n",
        "    idx = int(list(filter(str.isdigit, matric_number))[-1])%2\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Matric Number:', matric_number)\n",
        "        print('Project      : 1A-Classification')\n",
        "        print('https://colab.research.google.com/drive/1v-LiHThlLFF7bm8n64nh8lieAM2jA7lz?usp=sharing')\n",
        "    elif idx == 1: \n",
        "        print('Matric Number: ', matric_number)\n",
        "        print('Project      : 1B-Regression')\n",
        "        print('https://colab.research.google.com/drive/1Psi1UXp53t3_fMj8t0uIu4rubbV2wRCl?usp=sharing')\n",
        "\n",
        "except IndexError:\n",
        "    print(\"Please enter valid matric number and try again\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matric Number: A0150102H\n",
            "Project      : 1A-Classification\n",
            "https://colab.research.google.com/drive/1v-LiHThlLFF7bm8n64nh8lieAM2jA7lz?usp=sharing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVFfA7vnrFmX",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "**EE4305 (2020): Exercise 1A: Classification using MNIST dataset**\n",
        "\n",
        "---\n",
        "In this exercise you will learn basics of implementing an ANN-based classification model using TensorFlow and Keras libraries in Python. Following are the learning outcomes of this exercise:\n",
        "\n",
        "*   Ability to implement a neural network based classification model in Keras\n",
        "*   Train and predict using the model\n",
        "*   Evaluate model performance\n",
        "*   Understand the effect of various model parameters\n",
        "*   Propose means to improve model performance\n",
        "*   Perform data analytics and visualize outcomes \n",
        "\n",
        "Below the code skeleton is provided to help you with setting up the required options. Look for code cells describing the problems that each student needs to solve.\n",
        "\n",
        "More details about the dataset can be found [here](http://yann.lecun.com/exdb/mnist/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d72iCH0EVd0I",
        "colab_type": "text"
      },
      "source": [
        "### Instructions for using the Notebook\n",
        "\n",
        "If you have prior experience using Jupyter Notebooks, Colab is very similar. Please refer to the **Help** menu for FAQs. **Tools** menu provides useful information on commands and keyboard shortcuts. You may use the **Table of contents** menu on the left hand side to easily navigate between sections.\n",
        "\n",
        "---\n",
        "**VERY IMPORTANT: DO NOT MODIFY this file. Make a copy from the File menu and modify your version. You may use your matric number to name the file. For example, \"A0140410A: Exercise 1A - Classification\"**\n",
        "\n",
        "---\n",
        "\n",
        "The code skeleton provided is just a template. Feel free to modify/ improve any of the cells and scripts if you would prefer to implement them differently. If you have a local notebook environment with required packages, you may also download the notebook from **File** menu and run the script locally. Feel free to input additional **code** cells or **text** cells to present your solutions.\n",
        "\n",
        "### Submission\n",
        "\n",
        "For submission, download the notebook in '*.ipynb' format and submit in LumiNUS. Make sure all outputs are present under each cells (Especially for solution cells). Ensure you name the file appropriately as mentioned above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYIMQF0DuBVN",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "Following libraries are required for the implementation. Additional libraries may be imported as needed for specific tasks.\n",
        "\n",
        "*   numpy, pandas: For data storage and manipulation\n",
        "*   matplotlib, seaborn: Plotting and visualization\n",
        "*   tensorflow: Low-level library for machine learning implementations\n",
        "*   keras: High-level library to import datasets and implement ANN model\n",
        "*   sklearn: Data preprocessing, model evaluation metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBo5rUo5vfn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQtTgdgCvfVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmtRv32JvfFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2, L1L2\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcMPWuURvesj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score as skacc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-2csuc-wYkq",
        "colab_type": "text"
      },
      "source": [
        "### Load Data\n",
        "\n",
        "Keras provides an easy to use API to load popular machine learning datasets. For this problem we use the MNIST digits dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zFtKqAlDvlv",
        "colab_type": "code",
        "outputId": "bbecb05d-9220-4c3b-a0a3-0da2a91a2367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "source": [
        "(xtr, ytr), (xte, yte) = mnist.load_data()\n",
        "\n",
        "print('Data Shapes')\n",
        "print('Training data - xtr:{}, ytr:{}'.format(xtr.shape, ytr.shape))\n",
        "print('Testing data - xte:{}, yte:{}\\n'.format(xte.shape, yte.shape))\n",
        "\n",
        "# Visualize random samples\n",
        "\n",
        "f = plt.figure(figsize=(20,5))\n",
        "\n",
        "for i in range(16):\n",
        "    plt.subplot(2,8,i+1)\n",
        "    plt.imshow(xtr[np.random.randint(0,60000)], cmap='gray')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Shapes\n",
            "Training data - xtr:(60000, 28, 28), ytr:(60000,)\n",
            "Testing data - xte:(10000, 28, 28), yte:(10000,)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYHDDLPrFwAU",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "Input features are images of size 28 x 28. For ANN the data should be reshaped into one dimensional vectors. Each pixel in the input image is represented using grayscale values ranging from 0 to 255.\n",
        "\n",
        "Output labels indicate the digit value ranging from 0 to 9. However, for neural networks classification the data needs to be one-hot encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzY5sAlT0V79",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Problem I\n",
        "\n",
        "1.   What is the correct range of the input values for normalization? Implement normalization code in the cell below. \n",
        "\n",
        "```\n",
        "Hint: refer to sklearn.preprocessing - StandardScaler, MinMaxScaler.\n",
        "```\n",
        "\n",
        "2.   Why ANN requires output to be one-hot encoded? Implement code to convert output y to one-hot encoding.\n",
        "\n",
        "```\n",
        "Hint: refer to tensorflow.keras.utils.to_categorical\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC3zmBPh11Dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code for input data normalization and output one-hot encoding\n",
        "# You may directly use available helper functions from sklearn and keras or implement your own function\n",
        "\n",
        "xtr = xtr.reshape(-1, 784)\n",
        "xte = xte.reshape(-1, 784)\n",
        "scaler = MinMaxScaler()\n",
        "xtr_norm = scaler.fit_transform(xtr)\n",
        "xte_norm = scaler.transform(xte)\n",
        "\n",
        "ytr_cat = to_categorical(ytr,num_classes=10)\n",
        "yte_cat = to_categorical(yte,num_classes=10)\n",
        "\n",
        "\n",
        "# xtr_norm = \n",
        "# xte_norm = \n",
        "# ytr_cat = \n",
        "# yte_cat = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuHakLI13Yod",
        "colab_type": "text"
      },
      "source": [
        "### Define Model\n",
        "\n",
        "Below we define a baseline ANN-based model. Dense layers are fully connected layers in Keras used to define a standard MLP. Notice that the output layer contains 10 neurons which is equivalent to the number of classes. Output uses 'softmax' activation as this is a classification problem.\n",
        "\n",
        "\n",
        "Number of layers, neurons and activations for hidden layers can be customized\n",
        "\n",
        "The function returns a Keras model that can be used to train, predict, and evaluate using in-built functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW6D8p90VxIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(input_shape):\n",
        "  input_data = Input(shape=input_shape)\n",
        "\n",
        "  d1 = Dense(8, activation='tanh')(input_data)\n",
        "  d2 = Dense(16, activation='tanh')(d1)\n",
        "  # dr2 = Dropout(0.5)(d2)\n",
        "  d3 = Dense(10, activation='softmax')(d2)\n",
        "\n",
        "  model = Model(inputs=[input_data], outputs=[d3])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLXUdeMr4acN",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Problem II\n",
        "\n",
        "1.   Compile and train an ANN-model using in-built functions in Keras.\n",
        "\n",
        "```\n",
        "# Use compile to set following parameters:\n",
        "#  - optimizer = adam, sgd, rmsprop\n",
        "#  - loss = categorical_crossentropy\n",
        "\n",
        "compile(\n",
        "    optimizer='rmsprop', loss=None, metrics=None, loss_weights=None,\n",
        "    sample_weight_mode=None, weighted_metrics=None, **kwargs\n",
        "    )\n",
        "\n",
        "# Use fit to train the model. Configure following parameters:\n",
        "#  - x,y: input data and label\n",
        "#  - batch_size: minibatch size, for example, 32, 64, 100, 256\n",
        "#  - epochs: between 10-100\n",
        "#  - validation_split: percentage of training data for validation (eg. 0.2)\n",
        "\n",
        "fit(\n",
        "    x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None,\n",
        "    validation_split=0.0, validation_data=None, shuffle=True, class_weight=None,\n",
        "    sample_weight=None, initial_epoch=0, steps_per_epoch=None,\n",
        "    validation_steps=None, validation_batch_size=None, validation_freq=1,\n",
        "    max_queue_size=10, workers=1, use_multiprocessing=False\n",
        ")\n",
        "```\n",
        "[Compile API](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile)\n",
        "\n",
        "[Fit API](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_nVxQGA5QD",
        "colab_type": "text"
      },
      "source": [
        "2.   Predict output for test data using trained model and evaluate model performance.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Use predict function. Most parameters can be left as default. Pass normalized input features for output prediction\n",
        "\n",
        "predict(\n",
        "    x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, \n",
        "    workers=1, use_multiprocessing=False\n",
        ")\n",
        "\n",
        "```\n",
        "\n",
        "[Predict API](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict)\n",
        "\n",
        "The output from the model is one-hot encoded. How would you convert the prediction back to digit values?\n",
        "\n",
        "For example:\n",
        "\n",
        "```\n",
        "# Categorical to label is obtained by identifying index of max value in each row. HINT: numpy.argmax().\n",
        "\n",
        "yte_pred_cat = [[0,0,0,1,0,0,0,0,0,0],\n",
        "                [0,0,0,0,0,0,1,0,0,0]]\n",
        "yte_pred = [3,6]\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3NcQv9VWBTp",
        "colab_type": "code",
        "outputId": "959ad70f-c010-47ad-dbd3-0de934a7fda2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Code for model training here\n",
        "\n",
        "model = get_model(xtr_norm.shape[1:])\n",
        "model.summary()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "model.fit(\n",
        "    x=xtr_norm, y=ytr_cat, batch_size=32, epochs=20, verbose=1,\n",
        "    validation_split=0.2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_13 (InputLayer)        [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 8)                 6280      \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 16)                144       \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 6,594\n",
            "Trainable params: 6,594\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.6254 - val_loss: 0.3266\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3137 - val_loss: 0.2787\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2768 - val_loss: 0.2692\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2583 - val_loss: 0.2601\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2453 - val_loss: 0.2510\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2345 - val_loss: 0.2514\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2266 - val_loss: 0.2523\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2186 - val_loss: 0.2449\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2132 - val_loss: 0.2357\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2060 - val_loss: 0.2375\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2009 - val_loss: 0.2382\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.1970 - val_loss: 0.2376\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1964 - val_loss: 0.2284\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.1898 - val_loss: 0.2487\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.1885 - val_loss: 0.2345\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.1860 - val_loss: 0.2314\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.1851 - val_loss: 0.2423\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.1821 - val_loss: 0.2334\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.1799 - val_loss: 0.2342\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.1790 - val_loss: 0.2273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1077d402b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ra0s_4qWwnh",
        "colab_type": "code",
        "outputId": "19ee9c9d-81a9-47c8-a67d-ffdf8e0f782c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Code for prediction here\n",
        "\n",
        "yte_pred_cat = model.predict(xte_norm)\n",
        "yte_pred = np.argmax(yte_pred_cat,axis=1)\n",
        "accuracy = skacc(yte, yte_pred)\n",
        "print(\"Baseline model accuracy: \", accuracy)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline model accuracy:  0.9353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSqJ8SfZE3-x",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Problem III\n",
        "\n",
        "The baseline model is expected to have a test accuracy of around 93%. Can you propose at least 3 methods to improve the model to achieve better performance? Implement the 3 variants and present your results in the cell below.\n",
        "\n",
        "Explain why the model performs better/ worse than baseline.\n",
        "\n",
        "```\n",
        "Hint: Following are some of the model configurations that can be modified:\n",
        "```\n",
        "\n",
        "1.   Batch size, number of epochs, validation split, optimizer, learning rate\n",
        "2.   Model depth, dropout layers, activations, regularizations\n",
        "3.   Advanced: Model callbacks to control early stopping, learning rate decay, model checkpoints\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DBI7GOzGDPl",
        "colab_type": "text"
      },
      "source": [
        "### Solution here\n",
        "\n",
        "| Model         | Parameters Tuned                            | Accuracy  |\n",
        "| ------------- |---------------------------------------------| ---------:|\n",
        "| Baseline      | Batch Size and epochs                       | 0.939     |\n",
        "| Aplha         | Change Model Architecture                   | 0.936     |\n",
        "| Beta          | Regularization and Optimizer          | 0.90     |\n",
        "\n",
        "```\n",
        "# Please remove the examples and update with your own model variants.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdPcbNm0hjDO",
        "colab_type": "text"
      },
      "source": [
        "### Method one : Change the batch_size= 64 and epochs = 80\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKTJmr94HjDA",
        "colab_type": "code",
        "outputId": "227f944c-427f-4f6f-f9f3-12b3d0d71aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "### Modified model scripts and parameter details here\n",
        "### You may also use model.summary() to show the model architecture \n",
        "### for each variants\n",
        "# Code for model training here\n",
        "\n",
        "\n",
        "\n",
        "model = get_model(xtr_norm.shape[1:])\n",
        "model.summary()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',)\n",
        "hist=model.fit(\n",
        "    x=xtr_norm, y=ytr_cat, batch_size=64, epochs=80, verbose=1,\n",
        "    validation_split=0.2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "yte_pred_cat = model.predict(xte_norm)\n",
        "yte_pred = np.argmax(yte_pred_cat,axis=1)\n",
        "accuracy = skacc(yte, yte_pred)\n",
        "print(\"Baseline model accuracy: \", accuracy)\n",
        "\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_14 (InputLayer)        [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 8)                 6280      \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 16)                144       \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 6,594\n",
            "Trainable params: 6,594\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/80\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.8250 - val_loss: 0.4293\n",
            "Epoch 2/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.3909 - val_loss: 0.3413\n",
            "Epoch 3/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.3263 - val_loss: 0.3066\n",
            "Epoch 4/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.2962 - val_loss: 0.2889\n",
            "Epoch 5/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.2772 - val_loss: 0.2765\n",
            "Epoch 6/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.2632 - val_loss: 0.2654\n",
            "Epoch 7/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.2528 - val_loss: 0.2592\n",
            "Epoch 8/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.2444 - val_loss: 0.2602\n",
            "Epoch 9/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.2379 - val_loss: 0.2567\n",
            "Epoch 10/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.2305 - val_loss: 0.2549\n",
            "Epoch 11/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.2255 - val_loss: 0.2510\n",
            "Epoch 12/80\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 0.2210 - val_loss: 0.2442\n",
            "Epoch 13/80\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 0.2164 - val_loss: 0.2410\n",
            "Epoch 14/80\n",
            "750/750 [==============================] - 3s 3ms/step - loss: 0.2122 - val_loss: 0.2416\n",
            "Epoch 15/80\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.2100 - val_loss: 0.2414\n",
            "Epoch 16/80\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.2073 - val_loss: 0.2404\n",
            "Epoch 17/80\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.2046 - val_loss: 0.2428\n",
            "Epoch 18/80\n",
            "750/750 [==============================] - 2s 2ms/step - loss: 0.2022 - val_loss: 0.2410\n",
            "Epoch 19/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.2011 - val_loss: 0.2422\n",
            "Epoch 20/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1981 - val_loss: 0.2434\n",
            "Epoch 21/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1964 - val_loss: 0.2318\n",
            "Epoch 22/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1948 - val_loss: 0.2405\n",
            "Epoch 23/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1916 - val_loss: 0.2371\n",
            "Epoch 24/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1910 - val_loss: 0.2361\n",
            "Epoch 25/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1900 - val_loss: 0.2422\n",
            "Epoch 26/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1875 - val_loss: 0.2380\n",
            "Epoch 27/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1869 - val_loss: 0.2339\n",
            "Epoch 28/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1854 - val_loss: 0.2421\n",
            "Epoch 29/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1846 - val_loss: 0.2363\n",
            "Epoch 30/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1823 - val_loss: 0.2348\n",
            "Epoch 31/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1815 - val_loss: 0.2349\n",
            "Epoch 32/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1818 - val_loss: 0.2348\n",
            "Epoch 33/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1794 - val_loss: 0.2375\n",
            "Epoch 34/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1794 - val_loss: 0.2390\n",
            "Epoch 35/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1778 - val_loss: 0.2486\n",
            "Epoch 36/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1759 - val_loss: 0.2373\n",
            "Epoch 37/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1757 - val_loss: 0.2417\n",
            "Epoch 38/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1744 - val_loss: 0.2409\n",
            "Epoch 39/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1740 - val_loss: 0.2387\n",
            "Epoch 40/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1735 - val_loss: 0.2447\n",
            "Epoch 41/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1725 - val_loss: 0.2439\n",
            "Epoch 42/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1718 - val_loss: 0.2422\n",
            "Epoch 43/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1714 - val_loss: 0.2507\n",
            "Epoch 44/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1705 - val_loss: 0.2423\n",
            "Epoch 45/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1691 - val_loss: 0.2457\n",
            "Epoch 46/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1684 - val_loss: 0.2485\n",
            "Epoch 47/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1681 - val_loss: 0.2485\n",
            "Epoch 48/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1666 - val_loss: 0.2505\n",
            "Epoch 49/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1675 - val_loss: 0.2476\n",
            "Epoch 50/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1653 - val_loss: 0.2479\n",
            "Epoch 51/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1647 - val_loss: 0.2601\n",
            "Epoch 52/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1654 - val_loss: 0.2460\n",
            "Epoch 53/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1638 - val_loss: 0.2490\n",
            "Epoch 54/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1629 - val_loss: 0.2487\n",
            "Epoch 55/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1626 - val_loss: 0.2625\n",
            "Epoch 56/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1629 - val_loss: 0.2434\n",
            "Epoch 57/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1604 - val_loss: 0.2549\n",
            "Epoch 58/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1622 - val_loss: 0.2553\n",
            "Epoch 59/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1614 - val_loss: 0.2514\n",
            "Epoch 60/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1592 - val_loss: 0.2525\n",
            "Epoch 61/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1584 - val_loss: 0.2490\n",
            "Epoch 62/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1596 - val_loss: 0.2512\n",
            "Epoch 63/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1578 - val_loss: 0.2554\n",
            "Epoch 64/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1578 - val_loss: 0.2523\n",
            "Epoch 65/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1570 - val_loss: 0.2615\n",
            "Epoch 66/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1572 - val_loss: 0.2593\n",
            "Epoch 67/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1574 - val_loss: 0.2547\n",
            "Epoch 68/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1567 - val_loss: 0.2565\n",
            "Epoch 69/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1546 - val_loss: 0.2562\n",
            "Epoch 70/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1550 - val_loss: 0.2550\n",
            "Epoch 71/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1546 - val_loss: 0.2530\n",
            "Epoch 72/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1556 - val_loss: 0.2552\n",
            "Epoch 73/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1543 - val_loss: 0.2609\n",
            "Epoch 74/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1526 - val_loss: 0.2614\n",
            "Epoch 75/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1530 - val_loss: 0.2653\n",
            "Epoch 76/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1526 - val_loss: 0.2594\n",
            "Epoch 77/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1516 - val_loss: 0.2581\n",
            "Epoch 78/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1521 - val_loss: 0.2627\n",
            "Epoch 79/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1507 - val_loss: 0.2622\n",
            "Epoch 80/80\n",
            "750/750 [==============================] - 1s 2ms/step - loss: 0.1510 - val_loss: 0.2575\n",
            "Baseline model accuracy:  0.9316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaj_nJGIkYDW",
        "colab_type": "text"
      },
      "source": [
        "### Method two : Change the model architecture by adding two more dense layers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqOFdwonhqmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def method_two(input_shape):\n",
        "  input_data = Input(shape=input_shape)\n",
        "\n",
        "  d1 = Dense(8, activation='tanh')(input_data)\n",
        "  # dr1 = Dropout(0.2)(d1)\n",
        "  d2 = Dense(16, activation='tanh')(d1)\n",
        "  # dr2 = Dropout(0.5)(d2)\n",
        "  d3 = Dense(16,activation='tanh')(d2)\n",
        "  # dr3 = Dropout(0.5)(d3)\n",
        "  d4 = Dense(16,activation='tanh')(d3)\n",
        "  d5 = Dense(10, activation='softmax')(d4)\n",
        "\n",
        "  model = Model(inputs=[input_data], outputs=[d5])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JqbmGY8hq3L",
        "colab_type": "code",
        "outputId": "9892ec26-ee5a-4c52-d900-604eff6eac43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = method_two(xtr_norm.shape[1:])\n",
        "model.summary()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',)\n",
        "hist=model.fit(\n",
        "    x=xtr_norm, y=ytr_cat, batch_size=128, epochs=80, verbose=1,\n",
        "    validation_split=0.2)\n",
        "\n",
        "\n",
        "yte_pred_cat = model.predict(xte_norm)\n",
        "yte_pred = np.argmax(yte_pred_cat,axis=1)\n",
        "accuracy = skacc(yte, yte_pred)\n",
        "print(\"Baseline model accuracy: \", accuracy)\n",
        "\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_15 (InputLayer)        [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 8)                 6280      \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 16)                144       \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 7,138\n",
            "Trainable params: 7,138\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.9709 - val_loss: 0.5016\n",
            "Epoch 2/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.4419 - val_loss: 0.3783\n",
            "Epoch 3/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3577 - val_loss: 0.3159\n",
            "Epoch 4/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3128 - val_loss: 0.2941\n",
            "Epoch 5/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2845 - val_loss: 0.2756\n",
            "Epoch 6/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2634 - val_loss: 0.2531\n",
            "Epoch 7/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2481 - val_loss: 0.2435\n",
            "Epoch 8/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2356 - val_loss: 0.2412\n",
            "Epoch 9/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2273 - val_loss: 0.2336\n",
            "Epoch 10/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2196 - val_loss: 0.2338\n",
            "Epoch 11/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2132 - val_loss: 0.2249\n",
            "Epoch 12/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2071 - val_loss: 0.2318\n",
            "Epoch 13/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2025 - val_loss: 0.2323\n",
            "Epoch 14/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1985 - val_loss: 0.2287\n",
            "Epoch 15/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1950 - val_loss: 0.2211\n",
            "Epoch 16/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1923 - val_loss: 0.2189\n",
            "Epoch 17/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1886 - val_loss: 0.2234\n",
            "Epoch 18/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1847 - val_loss: 0.2174\n",
            "Epoch 19/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1831 - val_loss: 0.2217\n",
            "Epoch 20/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1792 - val_loss: 0.2158\n",
            "Epoch 21/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1772 - val_loss: 0.2161\n",
            "Epoch 22/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1733 - val_loss: 0.2203\n",
            "Epoch 23/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1724 - val_loss: 0.2185\n",
            "Epoch 24/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1711 - val_loss: 0.2160\n",
            "Epoch 25/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1697 - val_loss: 0.2189\n",
            "Epoch 26/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1658 - val_loss: 0.2235\n",
            "Epoch 27/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1663 - val_loss: 0.2164\n",
            "Epoch 28/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1643 - val_loss: 0.2175\n",
            "Epoch 29/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1628 - val_loss: 0.2134\n",
            "Epoch 30/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1607 - val_loss: 0.2158\n",
            "Epoch 31/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1593 - val_loss: 0.2142\n",
            "Epoch 32/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1588 - val_loss: 0.2142\n",
            "Epoch 33/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1572 - val_loss: 0.2223\n",
            "Epoch 34/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1563 - val_loss: 0.2109\n",
            "Epoch 35/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1566 - val_loss: 0.2228\n",
            "Epoch 36/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1542 - val_loss: 0.2173\n",
            "Epoch 37/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1516 - val_loss: 0.2163\n",
            "Epoch 38/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1512 - val_loss: 0.2283\n",
            "Epoch 39/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1502 - val_loss: 0.2135\n",
            "Epoch 40/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1510 - val_loss: 0.2122\n",
            "Epoch 41/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1476 - val_loss: 0.2126\n",
            "Epoch 42/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1469 - val_loss: 0.2135\n",
            "Epoch 43/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1469 - val_loss: 0.2182\n",
            "Epoch 44/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1463 - val_loss: 0.2246\n",
            "Epoch 45/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1444 - val_loss: 0.2180\n",
            "Epoch 46/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1435 - val_loss: 0.2172\n",
            "Epoch 47/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1429 - val_loss: 0.2136\n",
            "Epoch 48/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1418 - val_loss: 0.2199\n",
            "Epoch 49/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1400 - val_loss: 0.2158\n",
            "Epoch 50/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1400 - val_loss: 0.2138\n",
            "Epoch 51/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1389 - val_loss: 0.2239\n",
            "Epoch 52/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1392 - val_loss: 0.2196\n",
            "Epoch 53/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1376 - val_loss: 0.2183\n",
            "Epoch 54/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1375 - val_loss: 0.2264\n",
            "Epoch 55/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1366 - val_loss: 0.2135\n",
            "Epoch 56/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1365 - val_loss: 0.2240\n",
            "Epoch 57/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1348 - val_loss: 0.2253\n",
            "Epoch 58/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1341 - val_loss: 0.2337\n",
            "Epoch 59/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1357 - val_loss: 0.2187\n",
            "Epoch 60/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1327 - val_loss: 0.2232\n",
            "Epoch 61/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1314 - val_loss: 0.2199\n",
            "Epoch 62/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1321 - val_loss: 0.2260\n",
            "Epoch 63/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1304 - val_loss: 0.2269\n",
            "Epoch 64/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1298 - val_loss: 0.2219\n",
            "Epoch 65/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1306 - val_loss: 0.2274\n",
            "Epoch 66/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1302 - val_loss: 0.2236\n",
            "Epoch 67/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1286 - val_loss: 0.2249\n",
            "Epoch 68/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1307 - val_loss: 0.2294\n",
            "Epoch 69/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1284 - val_loss: 0.2217\n",
            "Epoch 70/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1287 - val_loss: 0.2245\n",
            "Epoch 71/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1260 - val_loss: 0.2233\n",
            "Epoch 72/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1257 - val_loss: 0.2260\n",
            "Epoch 73/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1262 - val_loss: 0.2307\n",
            "Epoch 74/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1266 - val_loss: 0.2283\n",
            "Epoch 75/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.2287\n",
            "Epoch 76/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1249 - val_loss: 0.2343\n",
            "Epoch 77/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1236 - val_loss: 0.2300\n",
            "Epoch 78/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1243 - val_loss: 0.2357\n",
            "Epoch 79/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1240 - val_loss: 0.2279\n",
            "Epoch 80/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1229 - val_loss: 0.2286\n",
            "Baseline model accuracy:  0.937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03ihztprwkqg",
        "colab_type": "text"
      },
      "source": [
        "### Method three : Change the model architecture by adding the regularizer and change the optimizer to 'SGD'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vByXL_Hlhq1E",
        "colab_type": "code",
        "outputId": "faf8d325-6a3f-48c6-d427-a81c922b72a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def method_three(input_shape):\n",
        "  input_data = Input(shape=input_shape)\n",
        "\n",
        "  d1 = Dense(8, activation='tanh')(input_data)\n",
        "  # dr1 = Dropout(0.2)(d1)\n",
        "  d2 = Dense(16, activation='tanh',kernel_regularizer=tf.keras.regularizers.l1(0.02),\n",
        "                              activity_regularizer=tf.keras.regularizers.l2(0.02))(d1)\n",
        "  # dr2 = Dropout(0.5)(d2)\n",
        "  d3 = Dense(16,activation='tanh',kernel_regularizer=tf.keras.regularizers.l1(0.02),\n",
        "                              activity_regularizer=tf.keras.regularizers.l2(0.02))(d2)\n",
        "  # dr3 = Dropout(0.5)(d3)\n",
        "  d4 = Dense(16,activation='tanh',kernel_regularizer=tf.keras.regularizers.l1(0.02),\n",
        "                              activity_regularizer=tf.keras.regularizers.l2(0.02))(d3)\n",
        "  d5 = Dense(10, activation='softmax')(d4)\n",
        "\n",
        "  model = Model(inputs=[input_data], outputs=[d5])\n",
        "\n",
        "  return model\n",
        "\n",
        "model = method_three(xtr_norm.shape[1:])\n",
        "model.summary()\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',)\n",
        "history=model.fit(\n",
        "    x=xtr_norm, y=ytr_cat, batch_size=128, epochs=80, verbose=1,\n",
        "    validation_split=0.2)\n",
        "\n",
        "\n",
        "yte_pred_cat = model.predict(xte_norm)\n",
        "yte_pred = np.argmax(yte_pred_cat,axis=1)\n",
        "accuracy = skacc(yte, yte_pred)\n",
        "print(\"Baseline model accuracy: \", accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_16 (InputLayer)        [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 8)                 6280      \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 16)                144       \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 7,138\n",
            "Trainable params: 7,138\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 4.4669 - val_loss: 3.9383\n",
            "Epoch 2/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 3.5808 - val_loss: 3.2539\n",
            "Epoch 3/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 3.0204 - val_loss: 2.7910\n",
            "Epoch 4/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 2.6341 - val_loss: 2.4780\n",
            "Epoch 5/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 2.3715 - val_loss: 2.2556\n",
            "Epoch 6/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 2.1757 - val_loss: 2.0833\n",
            "Epoch 7/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 2.0260 - val_loss: 1.9554\n",
            "Epoch 8/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.9143 - val_loss: 1.8577\n",
            "Epoch 9/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.8270 - val_loss: 1.7792\n",
            "Epoch 10/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.7546 - val_loss: 1.7120\n",
            "Epoch 11/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.6937 - val_loss: 1.6561\n",
            "Epoch 12/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.6418 - val_loss: 1.6071\n",
            "Epoch 13/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.5978 - val_loss: 1.5659\n",
            "Epoch 14/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.5572 - val_loss: 1.5258\n",
            "Epoch 15/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.5184 - val_loss: 1.4873\n",
            "Epoch 16/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.4820 - val_loss: 1.4524\n",
            "Epoch 17/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.4480 - val_loss: 1.4189\n",
            "Epoch 18/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.4144 - val_loss: 1.3868\n",
            "Epoch 19/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.3854 - val_loss: 1.3593\n",
            "Epoch 20/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.3580 - val_loss: 1.3320\n",
            "Epoch 21/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.3328 - val_loss: 1.3084\n",
            "Epoch 22/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.3095 - val_loss: 1.2862\n",
            "Epoch 23/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.2887 - val_loss: 1.2684\n",
            "Epoch 24/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.2708 - val_loss: 1.2503\n",
            "Epoch 25/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.2523 - val_loss: 1.2317\n",
            "Epoch 26/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.2348 - val_loss: 1.2145\n",
            "Epoch 27/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.2194 - val_loss: 1.2006\n",
            "Epoch 28/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.2045 - val_loss: 1.1864\n",
            "Epoch 29/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.1891 - val_loss: 1.1702\n",
            "Epoch 30/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.1737 - val_loss: 1.1554\n",
            "Epoch 31/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.1579 - val_loss: 1.1391\n",
            "Epoch 32/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.1402 - val_loss: 1.1215\n",
            "Epoch 33/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.1220 - val_loss: 1.1034\n",
            "Epoch 34/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.1036 - val_loss: 1.0850\n",
            "Epoch 35/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.0865 - val_loss: 1.0682\n",
            "Epoch 36/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.0713 - val_loss: 1.0542\n",
            "Epoch 37/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.0593 - val_loss: 1.0467\n",
            "Epoch 38/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.0487 - val_loss: 1.0340\n",
            "Epoch 39/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.0387 - val_loss: 1.0253\n",
            "Epoch 40/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.0298 - val_loss: 1.0172\n",
            "Epoch 41/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.0209 - val_loss: 1.0077\n",
            "Epoch 42/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.0123 - val_loss: 0.9988\n",
            "Epoch 43/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.0036 - val_loss: 0.9914\n",
            "Epoch 44/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9954 - val_loss: 0.9830\n",
            "Epoch 45/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9880 - val_loss: 0.9758\n",
            "Epoch 46/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9817 - val_loss: 0.9714\n",
            "Epoch 47/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9762 - val_loss: 0.9650\n",
            "Epoch 48/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9706 - val_loss: 0.9620\n",
            "Epoch 49/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9647 - val_loss: 0.9550\n",
            "Epoch 50/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.9594 - val_loss: 0.9503\n",
            "Epoch 51/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9542 - val_loss: 0.9462\n",
            "Epoch 52/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9492 - val_loss: 0.9406\n",
            "Epoch 53/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9441 - val_loss: 0.9369\n",
            "Epoch 54/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9393 - val_loss: 0.9323\n",
            "Epoch 55/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9346 - val_loss: 0.9284\n",
            "Epoch 56/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9300 - val_loss: 0.9241\n",
            "Epoch 57/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9257 - val_loss: 0.9199\n",
            "Epoch 58/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9214 - val_loss: 0.9154\n",
            "Epoch 59/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9174 - val_loss: 0.9118\n",
            "Epoch 60/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9134 - val_loss: 0.9127\n",
            "Epoch 61/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.9097 - val_loss: 0.9048\n",
            "Epoch 62/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9058 - val_loss: 0.9031\n",
            "Epoch 63/80\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.9023 - val_loss: 0.8991\n",
            "Epoch 64/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8987 - val_loss: 0.8948\n",
            "Epoch 65/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8952 - val_loss: 0.8930\n",
            "Epoch 66/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8920 - val_loss: 0.8904\n",
            "Epoch 67/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8889 - val_loss: 0.8860\n",
            "Epoch 68/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8856 - val_loss: 0.8862\n",
            "Epoch 69/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8825 - val_loss: 0.8810\n",
            "Epoch 70/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8796 - val_loss: 0.8787\n",
            "Epoch 71/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8768 - val_loss: 0.8769\n",
            "Epoch 72/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8739 - val_loss: 0.8741\n",
            "Epoch 73/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8713 - val_loss: 0.8708\n",
            "Epoch 74/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8684 - val_loss: 0.8688\n",
            "Epoch 75/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8660 - val_loss: 0.8671\n",
            "Epoch 76/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8632 - val_loss: 0.8631\n",
            "Epoch 77/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8608 - val_loss: 0.8613\n",
            "Epoch 78/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8583 - val_loss: 0.8617\n",
            "Epoch 79/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8558 - val_loss: 0.8588\n",
            "Epoch 80/80\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8533 - val_loss: 0.8561\n",
            "Baseline model accuracy:  0.8882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An5b01s7hqzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe232YDthqx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s6LZn1ihqub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSI_3SZQIeRl",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Problem IV\n",
        "\n",
        "Accuracy score is the standard metric for evaluating model performance in classification problems. \n",
        "\n",
        "*   Can you propose other measures to evaluate model performance? Compute scores for your models using the proposed metric.\n",
        "*   What are the benefits of using the proposed metric?\n",
        "\n",
        "```\n",
        "HINT: Several popular metrics can be found in the sklearn.metrics library\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoTm1vrHi72j",
        "colab_type": "text"
      },
      "source": [
        "## Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8uv-6Zj5vZU",
        "colab_type": "text"
      },
      "source": [
        "Precision is the ratio of True Positive / (True Positive + False Positive)\n",
        "\n",
        "Recall is the ratio of True Positive / (True Positive + False Negative)\n",
        "\n",
        "f1_score = 2* Precision * Recall / (Precision + recall)\n",
        "\n",
        "In the example of lets say COVID-19 case detection. In this case, we would tolerate false positive(wrongly classified as patients). However, we would not tolerate false negative(as it may spread to more people).\n",
        "In this case, we need to focus or maximize our recall score even though the other scores may fall.\n",
        "\n",
        "However, a single score would not evaluate the performance entirely.\n",
        "In the case of unbalanced dataset where a single or a few labels have occupy large percentage of the dataset.\n",
        "\n",
        "For instance, a binary class classification problem. 99% of the data is from postive class and only 1% is from the negative class.\n",
        "In this case, if the model simply (dumbly) classify all data as positive class, the recall score itself can show 100% which seems to be perfect. However, it is not the case.\n",
        "In this example, other metrics should come in such as f1-score.\n",
        "\n",
        "Using the F1 score we can get a more realistic measure of our classifier's performance. Moreover we can avoid to be fooled by the arithmetic mean between a very poor PRECISION and very high RECALL, which can be obtained simply by classifying all of the documents as positive using a dummy classifier.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW_jH75hwzNh",
        "colab_type": "text"
      },
      "source": [
        "Other methods like precision score, recall score, f1-score, macro-average and micro-average"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oADi3zgBJ0aB",
        "colab_type": "code",
        "outputId": "e4545c48-e32d-4dd7-b58f-e36d45e22640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "### Problem IV codes here. Add more cells as needed\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "accuracy = precision_score(yte, yte_pred,average='macro')\n",
        "print(\"Precision accuracy: \", accuracy)\n",
        "\n",
        "accuracy = recall_score(yte, yte_pred,average='micro')\n",
        "print(\"Recall accuracy: \", accuracy)\n",
        "\n",
        "accuracy = f1_score(yte, yte_pred,average='weighted')\n",
        "print(\"f1_score accuracy: \", accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision accuracy:  0.9245930027478039\n",
            "Recall accuracy:  0.9255\n",
            "f1_score accuracy:  0.9253510044019032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1JcJXl1JKsz",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Bonus\n",
        "\n",
        "Data visualization at various stage help develop intuition of model performance, identify cause for errors, and impelement ideas for improvement.\n",
        "\n",
        "*   Can you write scripts to visualize different aspects of the model?\n",
        "\n",
        "```\n",
        "HINT: You can visualize model loss over epochs using the keras model history returned by model.fit(). \n",
        "      Confusion matrix is a good way of identifying the most challenging classes.\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKdkt1WAJ7kQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "0026263d-1b89-4cfe-e080-29c855b49800"
      },
      "source": [
        "### Bonus codes here. Add more cells as needed\n",
        "plt.plot(hist.history['loss'],label='training loss')\n",
        "plt.plot(hist.history['val_loss'],label='validation loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1077fd7fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgP5zUxyZRPk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1b74c7d2-21fb-44fa-95b3-126ea0bf236a"
      },
      "source": [
        "labels = [x for x in range(0,10)]\n",
        "labels"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyYntmo5A-i3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "9bf36035-1437-44e2-f2f1-8d408fb042e1"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(yte,yte_pred)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "print(cm)\n",
        "colorbar =ax.matshow(cm)\n",
        "fig.colorbar(colorbar)\n",
        "plt.xticks(labels,labels)\n",
        "plt.yticks(labels,labels)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 944    0    6    1   11    8    2    5    3    0]\n",
            " [   0 1085    4    2    0    1    2    1   35    5]\n",
            " [  19    0  902   14    6   37   20    2   29    3]\n",
            " [   0    7   29  880    0   66    1   12   12    3]\n",
            " [   1    2    4    0  898    0   21    1    4   51]\n",
            " [  17    0   33   60    8  663   10   25   68    8]\n",
            " [  27    1   28    0   26    3  858    0   15    0]\n",
            " [   5    6   10   17    3   21    0  930    1   35]\n",
            " [   3   19    8   11    1   64   20    5  828   15]\n",
            " [   5   12    0    3   35   20    0   31    9  894]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'True')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9Rd6UEDokx1",
        "colab_type": "text"
      },
      "source": [
        "### Have a Feedback?\n",
        "\n",
        "If you have suggestions to improve this notebook, please share your ideas via email to the GA. We are always listening!\n",
        "\n",
        "Thank you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQMVEd0qkz7s",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "End of the Notebook\n",
        "\n",
        "---"
      ]
    }
  ]
}